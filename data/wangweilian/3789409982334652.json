[{"original_text": "Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "王威廉", "province": 400, "statuses_count": 2449, "friends_count": 337, "city": 1, "text": "Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "user_description": "NLP, Machine Learning, and Speech @ CMU http://www.cs.cmu.edu/~yww/", "mid": "3789409982334652", "verified_reason": "", "followers_count": 43807, "parent": "", "t": 1418949206, "gender": "m", "verified": false, "verified_type": -1, "uid": "1657470871"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "MessiApril", "province": 33, "statuses_count": 1738, "friends_count": 778, "city": 1, "text": "", "user_description": "", "mid": "3789410157683519", "verified_reason": "", "followers_count": 25, "parent": "3789409982334652", "t": 1418949247, "gender": "f", "verified": false, "verified_type": -1, "uid": "1869510614"}, {"original_text": "最后都是在解决bias vs var问题啊[思考]//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "尘绳聋", "province": 44, "statuses_count": 857, "friends_count": 430, "city": 3, "text": "最后都是在解决bias vs var问题啊[思考]", "user_description": "I am a weak learner in the progress of boosting.", "mid": "3789410514661612", "verified_reason": "", "followers_count": 1257, "parent": "3789409982334652", "t": 1418949333, "gender": "m", "verified": false, "verified_type": -1, "uid": "1254062861"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Aaron兰塞", "province": 31, "statuses_count": 1944, "friends_count": 222, "city": 12, "text": "", "user_description": "", "mid": "3789410631788497", "verified_reason": "", "followers_count": 44, "parent": "3789409982334652", "t": 1418949361, "gender": "m", "verified": false, "verified_type": -1, "uid": "1809118673"}, {"original_text": "@我的印象笔记 //@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "买买提混世小魔王公主殿下", "province": 33, "statuses_count": 293, "friends_count": 128, "city": 1, "text": "@我的印象笔记 ", "user_description": "做甘心被忘记的人 做不经意的微笑 不做刺痛的思念", "mid": "3789412263396783", "verified_reason": "", "followers_count": 144, "parent": "3789409982334652", "t": 1418949750, "gender": "f", "verified": false, "verified_type": -1, "uid": "1997939575"}, {"original_text": "//@尘绳聋-SYSU:最后都是在解决bias vs var问题啊[思考]//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "砰砰的小屋", "province": 32, "statuses_count": 22409, "friends_count": 1790, "city": 2, "text": "", "user_description": "", "mid": "3789412297202122", "verified_reason": "", "followers_count": 511, "parent": "3789410514661612", "t": 1418949757, "gender": "f", "verified": false, "verified_type": -1, "uid": "2036923815"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "tiger_bottom", "province": 11, "statuses_count": 9287, "friends_count": 861, "city": 1000, "text": "", "user_description": "程序员", "mid": "3789412367920895", "verified_reason": "", "followers_count": 692, "parent": "3789409982334652", "t": 1418949775, "gender": "m", "verified": false, "verified_type": -1, "uid": "1683020051"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "胆小蝎子", "province": 32, "statuses_count": 797, "friends_count": 700, "city": 5, "text": "", "user_description": "微博微信微生物小材大用，分享围观凑热闹其乐融融。", "mid": "3789413396287881", "verified_reason": "", "followers_count": 113, "parent": "3789409982334652", "t": 1418950020, "gender": "m", "verified": false, "verified_type": -1, "uid": "1907984411"}, {"original_text": "//@尘绳聋-SYSU: 最后都是在解决bias vs var问题啊[思考]//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "lnas01", "province": 11, "statuses_count": 2121, "friends_count": 860, "city": 8, "text": "", "user_description": "", "mid": "3789413563594317", "verified_reason": "", "followers_count": 74, "parent": "3789410514661612", "t": 1418950060, "gender": "m", "verified": false, "verified_type": -1, "uid": "1994150517"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "徐振-HITNLP", "province": 37, "statuses_count": 380, "friends_count": 250, "city": 10, "text": "", "user_description": "", "mid": "3789414100559767", "verified_reason": "", "followers_count": 147, "parent": "3789409982334652", "t": 1418950188, "gender": "m", "verified": false, "verified_type": -1, "uid": "2078306387"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Yeah_Beautiful", "province": 11, "statuses_count": 997, "friends_count": 353, "city": 8, "text": "", "user_description": "keep fighting， and enjoy life", "mid": "3789414515498493", "verified_reason": "", "followers_count": 139, "parent": "3789409982334652", "t": 1418950287, "gender": "m", "verified": false, "verified_type": 220, "uid": "2302485114"}, {"original_text": "这两个东西都能让模型稳定，防过拟合，我在pullword上都用了，效果很好，只是调参负担加重了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:input加噪音相当于regularzation，dropout 解决的是co-adaption", "username": "梁斌penny", "province": 100, "statuses_count": 22342, "friends_count": 1192, "city": 1000, "text": "这两个东西都能让模型稳定，防过拟合，我在pullword上都用了，效果很好，只是调参负担加重了", "user_description": "个人主页（http://pennyliang.com/）", "mid": "3789414675020515", "verified_reason": "", "followers_count": 49443, "parent": "3789409982334652", "t": 1418950325, "gender": "m", "verified": false, "verified_type": -1, "uid": "1497035431"}, {"original_text": "//@梁斌penny: 这两个东西都能让模型稳定，防过拟合，我在pullword上都用了，效果很好，只是调参负担加重了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:input加噪音相当于regularzation，dropout 解决的", "username": "囧-Smith", "province": 11, "statuses_count": 1883, "friends_count": 141, "city": 8, "text": "", "user_description": "Dreamer of a bigger world", "mid": "3789415052573876", "verified_reason": "", "followers_count": 96, "parent": "3789414675020515", "t": 1418950415, "gender": "m", "verified": false, "verified_type": -1, "uid": "1832390083"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:input加噪音相当于regularzation，dropout 解决的是co-adaption", "username": "算法组", "province": 11, "statuses_count": 1392, "friends_count": 715, "city": 8, "text": "", "user_description": "suanfazu.com 算法技术社区。微信公众号：suanfa_zu 算法组。关注算法，机器学习，大数据，搜索引擎，移动互联网等IT技术。", "mid": "3789415761154188", "verified_reason": "", "followers_count": 10229, "parent": "3789414675020515", "t": 1418950584, "gender": "m", "verified": false, "verified_type": -1, "uid": "3183064657"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "vinW", "province": 44, "statuses_count": 10210, "friends_count": 2000, "city": 3, "text": " ", "user_description": "关注彩票业大数据、机器学习", "mid": "3789416407912601", "verified_reason": "北师大国家彩票发展研究院客座数据科学家 原英国利兹大学博士后", "followers_count": 18735, "parent": "3789409982334652", "t": 1418950738, "gender": "m", "verified": true, "verified_type": 0, "uid": "1541603965"}, {"original_text": "//@梁斌penny: 这两个东西都能让模型稳定，防过拟合，我在pullword上都用了，效果很好，只是调参负担加重了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:input加噪音相当于regularzation，dropout 解决的是", "username": "殆知阁", "province": 11, "statuses_count": 129226, "friends_count": 5000, "city": 1, "text": "", "user_description": "古代文献请移步http://wenxian.fanren8.com 。。。微博只刷屏。不刷了那是被禁言了。请移步@殆知阁2", "mid": "3789416700836218", "verified_reason": "", "followers_count": 150221, "parent": "3789414675020515", "t": 1418950808, "gender": "m", "verified": false, "verified_type": -1, "uid": "2301762597"}, {"original_text": "不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:", "username": "鲁东东胖", "province": 81, "statuses_count": 3375, "friends_count": 998, "city": 2, "text": "不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了", "user_description": "Noah's Ark Lab@HK. Looking for student interns on Deep Learning for NLP", "mid": "3789417359188903", "verified_reason": "", "followers_count": 4673, "parent": "3789409982334652", "t": 1418950965, "gender": "m", "verified": false, "verified_type": -1, "uid": "2141767181"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "huyanxing", "province": 100, "statuses_count": 7445, "friends_count": 396, "city": 1000, "text": " ", "user_description": "", "mid": "3789417632397357", "verified_reason": "", "followers_count": 525, "parent": "3789409982334652", "t": 1418951030, "gender": "m", "verified": false, "verified_type": -1, "uid": "1752802870"}, {"original_text": "//@鲁东东胖:不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:", "username": "刘知远THU", "province": 11, "statuses_count": 5189, "friends_count": 779, "city": 8, "text": "", "user_description": "http://nlp.csai.tsinghua.edu.cn/~lzy", "mid": "3789419209687105", "verified_reason": "清华大学计算机系助理研究员", "followers_count": 18259, "parent": "3789417359188903", "t": 1418951405, "gender": "m", "verified": true, "verified_type": 0, "uid": "1464484735"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "听松馆少主", "province": 11, "statuses_count": 3433, "friends_count": 166, "city": 8, "text": "", "user_description": "君子不器", "mid": "3789419222284507", "verified_reason": "", "followers_count": 276, "parent": "3789409982334652", "t": 1418951409, "gender": "m", "verified": false, "verified_type": -1, "uid": "2471068692"}, {"original_text": "//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "52nlp", "province": 11, "statuses_count": 15695, "friends_count": 1994, "city": 8, "text": "", "user_description": "Make something people want; A blog for fools written by fools", "mid": "3789419750233644", "verified_reason": "", "followers_count": 17880, "parent": "3789417359188903", "t": 1418951535, "gender": "m", "verified": false, "verified_type": 220, "uid": "2104931705"}, {"original_text": "//@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "蒋振超", "province": 37, "statuses_count": 1203, "friends_count": 303, "city": 3, "text": "", "user_description": "#7C7C7C", "mid": "3789419963776556", "verified_reason": "", "followers_count": 402, "parent": "3789419750233644", "t": 1418951586, "gender": "m", "verified": false, "verified_type": 220, "uid": "1916226047"}, {"original_text": "//@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "长翅膀的羊_nash", "province": 81, "statuses_count": 8829, "friends_count": 980, "city": 14, "text": "", "user_description": "data miner http://www.cse.ust.hk/~kxmo/", "mid": "3789420069056824", "verified_reason": "", "followers_count": 970, "parent": "3789419750233644", "t": 1418951611, "gender": "m", "verified": false, "verified_type": -1, "uid": "1872050035"}, {"original_text": "//@算法组://@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:input加噪音相当于regularzation，dropout 解决的是co-adaption", "username": "红黑合不说", "province": 44, "statuses_count": 2397, "friends_count": 754, "city": 1, "text": "", "user_description": "宅家的挨踢人", "mid": "3789420086330754", "verified_reason": "", "followers_count": 111, "parent": "3789415761154188", "t": 1418951614, "gender": "m", "verified": false, "verified_type": -1, "uid": "1415089192"}, {"original_text": "//@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "机器也有梦想", "province": 32, "statuses_count": 1244, "friends_count": 66, "city": 1, "text": "", "user_description": "执着地追求自己的梦想！欢迎访问我的个人主页:fxlnjfu.github.io", "mid": "3789420400464426", "verified_reason": "", "followers_count": 116, "parent": "3789419750233644", "t": 1418951691, "gender": "m", "verified": false, "verified_type": -1, "uid": "2292039803"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "光饱和点", "province": 11, "statuses_count": 11365, "friends_count": 811, "city": 8, "text": "", "user_description": "", "mid": "3789420739856766", "verified_reason": "", "followers_count": 60, "parent": "3789409982334652", "t": 1418951771, "gender": "m", "verified": false, "verified_type": -1, "uid": "3270346483"}, {"original_text": "//@52nlp://@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "LoveLife聪", "province": 33, "statuses_count": 3380, "friends_count": 491, "city": 1, "text": "", "user_description": "要毕业了～已卖身于三石哥，准备开始挖掘机生活！", "mid": "3789421034057720", "verified_reason": "", "followers_count": 195, "parent": "3789419750233644", "t": 1418951841, "gender": "m", "verified": false, "verified_type": -1, "uid": "1740652783"}, {"original_text": "//@梁斌penny: 这两个东西都能让模型稳定防过拟合，我在pullword上都用了效果很好，只是调参负担加重@王威廉: Hinton自己说是feature co-adaptation，不过2013年时候，已有NIPS文章指出dropout在GLM上等价于L2 regularization@梁斌penny:input加噪音相当于regularzation，dropout 解决的是co-adaption", "username": "anklebreaker11", "province": 31, "statuses_count": 4221, "friends_count": 194, "city": 1, "text": "", "user_description": "Keep it real", "mid": "3789421176286698", "verified_reason": "", "followers_count": 157, "parent": "3789414675020515", "t": 1418951875, "gender": "m", "verified": false, "verified_type": -1, "uid": "2445412693"}, {"original_text": "//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny:", "username": "黄瓜皮_SCU", "province": 51, "statuses_count": 2586, "friends_count": 197, "city": 1, "text": "", "user_description": "嘉培。茂名实验->茂名一中->四川大学->?", "mid": "3789421407014896", "verified_reason": "", "followers_count": 53, "parent": "3789417359188903", "t": 1418951930, "gender": "m", "verified": false, "verified_type": -1, "uid": "1774395687"}, {"original_text": "//@52nlp://@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "都金涛", "province": 31, "statuses_count": 236, "friends_count": 255, "city": 10, "text": "", "user_description": "IT小小鸟一只！", "mid": "3789422077963092", "verified_reason": "", "followers_count": 155, "parent": "3789419750233644", "t": 1418952090, "gender": "m", "verified": false, "verified_type": -1, "uid": "1657823741"}, {"original_text": "//@vinW: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "星空下的巫师", "province": 33, "statuses_count": 9132, "friends_count": 916, "city": 1, "text": "", "user_description": "计算机视觉，模式识别，机器学习，深度学习，*Shicai@东南大学，南京，中国~", "mid": "3789422107939465", "verified_reason": "", "followers_count": 4926, "parent": "3789416407912601", "t": 1418952096, "gender": "m", "verified": false, "verified_type": 220, "uid": "1785748853"}, {"original_text": "我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "winsty", "province": 81, "statuses_count": 2223, "friends_count": 294, "city": 14, "text": "我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析", "user_description": "HKUST CSE PhD, Computer Vision and Machine Learning", "mid": "3789422795710715", "verified_reason": "", "followers_count": 5952, "parent": "3789420069056824", "t": 1418952261, "gender": "m", "verified": false, "verified_type": -1, "uid": "1218274631"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "老师木", "province": 11, "statuses_count": 13202, "friends_count": 1366, "city": 1000, "text": "", "user_description": "", "mid": "3789423034434166", "verified_reason": "", "followers_count": 26584, "parent": "3789409982334652", "t": 1418952318, "gender": "m", "verified": false, "verified_type": -1, "uid": "1991303247"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "fanyangrourou", "province": 400, "statuses_count": 2385, "friends_count": 63, "city": 1, "text": "", "user_description": "不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。", "mid": "3789423361807234", "verified_reason": "", "followers_count": 258, "parent": "3789409982334652", "t": 1418952396, "gender": "f", "verified": false, "verified_type": -1, "uid": "1690041140"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "刘知远THU", "province": 11, "statuses_count": 5189, "friends_count": 779, "city": 8, "text": "", "user_description": "http://nlp.csai.tsinghua.edu.cn/~lzy", "mid": "3789423407374682", "verified_reason": "清华大学计算机系助理研究员", "followers_count": 18259, "parent": "3789422795710715", "t": 1418952407, "gender": "m", "verified": true, "verified_type": 0, "uid": "1464484735"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "_胡金水_", "province": 34, "statuses_count": 2700, "friends_count": 651, "city": 1, "text": "", "user_description": "", "mid": "3789423521513337", "verified_reason": "", "followers_count": 422, "parent": "3789423034434166", "t": 1418952434, "gender": "m", "verified": false, "verified_type": -1, "uid": "1902694697"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "强闰伟_LocalsNake", "province": 11, "statuses_count": 1697, "friends_count": 485, "city": 8, "text": "", "user_description": "runwei.info", "mid": "3789423613358020", "verified_reason": "", "followers_count": 1075, "parent": "3789409982334652", "t": 1418952455, "gender": "m", "verified": false, "verified_type": 220, "uid": "1931740753"}, {"original_text": "//@老师木: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "仉佃星_zhangdianx", "province": 21, "statuses_count": 808, "friends_count": 252, "city": 1, "text": "", "user_description": "", "mid": "3789423742978989", "verified_reason": "", "followers_count": 183, "parent": "3789423034434166", "t": 1418952487, "gender": "m", "verified": false, "verified_type": -1, "uid": "1919933992"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "鲁东东胖", "province": 81, "statuses_count": 3375, "friends_count": 998, "city": 2, "text": "", "user_description": "Noah's Ark Lab@HK. Looking for student interns on Deep Learning for NLP", "mid": "3789423856244577", "verified_reason": "", "followers_count": 4673, "parent": "3789422795710715", "t": 1418952514, "gender": "m", "verified": false, "verified_type": -1, "uid": "2141767181"}, {"original_text": " //@winsty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "Juggernaut_One", "province": 81, "statuses_count": 707, "friends_count": 284, "city": 1, "text": " ", "user_description": "NeuronMatrix Capital Management", "mid": "3789424209291131", "verified_reason": "", "followers_count": 754, "parent": "3789422795710715", "t": 1418952598, "gender": "m", "verified": false, "verified_type": -1, "uid": "1707004311"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Fancy_-_", "province": 12, "statuses_count": 610, "friends_count": 133, "city": 4, "text": "", "user_description": "Forget&Forward", "mid": "3789424267517460", "verified_reason": "", "followers_count": 55, "parent": "3789409982334652", "t": 1418952612, "gender": "m", "verified": false, "verified_type": -1, "uid": "3686843257"}, {"original_text": "//@老师木: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "圣依克明安", "province": 21, "statuses_count": 1604, "friends_count": 675, "city": 1, "text": "", "user_description": "你想拥有你从未有过的东西，那么你必须去做你从未做过的事情！", "mid": "3789424301077181", "verified_reason": "", "followers_count": 101, "parent": "3789423034434166", "t": 1418952620, "gender": "m", "verified": false, "verified_type": -1, "uid": "2171702760"}, {"original_text": "证明dropout等价于L2的文章名是什么？ //@听松馆少主://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "梁堃0xFF", "province": 11, "statuses_count": 3824, "friends_count": 371, "city": 8, "text": "证明dropout等价于L2的文章名是什么？ ", "user_description": "::= λf. (λx. f (λy. x x y)) (λx. f (λy. x x y)) | Math's fan | ANN's fan | Programming Languages' fan", "mid": "3789424406307643", "verified_reason": "", "followers_count": 1585, "parent": "3789419222284507", "t": 1418952645, "gender": "m", "verified": false, "verified_type": 220, "uid": "1903358337"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "木乱人先生", "province": 11, "statuses_count": 1989, "friends_count": 132, "city": 8, "text": "", "user_description": "又是一年花开放，幸福变得好美", "mid": "3789425978834451", "verified_reason": "", "followers_count": 105, "parent": "3789422795710715", "t": 1418953020, "gender": "m", "verified": false, "verified_type": 220, "uid": "1746183900"}, {"original_text": "mSDA是个奇怪的方法。 //@winsty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex", "username": "cswhjiang", "province": 81, "statuses_count": 6998, "friends_count": 549, "city": 1, "text": "mSDA是个奇怪的方法。 ", "user_description": "blog: https://cswhjiang.github.io/", "mid": "3789426608222700", "verified_reason": "", "followers_count": 1301, "parent": "3789422795710715", "t": 1418953170, "gender": "m", "verified": false, "verified_type": -1, "uid": "2358675560"}, {"original_text": ": //@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "姚yaoqj", "province": 42, "statuses_count": 983, "friends_count": 251, "city": 1, "text": ": ", "user_description": "", "mid": "3789426687368866", "verified_reason": "", "followers_count": 136, "parent": "3789423407374682", "t": 1418953189, "gender": "m", "verified": false, "verified_type": -1, "uid": "2396267384"}, {"original_text": "//@老师木: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "姚yaoqj", "province": 42, "statuses_count": 983, "friends_count": 251, "city": 1, "text": "", "user_description": "", "mid": "3789427111237979", "verified_reason": "", "followers_count": 136, "parent": "3789423034434166", "t": 1418953290, "gender": "m", "verified": false, "verified_type": -1, "uid": "2396267384"}, {"original_text": "DAE 在Robust ASR上也不新了//@鲁东东胖: //@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析", "username": "黄浩XJU", "province": 65, "statuses_count": 2082, "friends_count": 347, "city": 1, "text": "DAE 在Robust ASR上也不新了", "user_description": "教书匠、码农、西北民间科学家黄大锤WonderTree https://sites.google.com/site/hwanghao", "mid": "3789427144627134", "verified_reason": "", "followers_count": 377, "parent": "3789423856244577", "t": 1418953298, "gender": "m", "verified": false, "verified_type": -1, "uid": "2166237637"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "吸吸卡", "province": 32, "statuses_count": 1876, "friends_count": 135, "city": 12, "text": "", "user_description": "叫兽 思想家 教育家 诺基亚 没U盘 喜迎十八大 处女男是什么情况", "mid": "3789428264440872", "verified_reason": "", "followers_count": 78, "parent": "3789409982334652", "t": 1418953565, "gender": "m", "verified": false, "verified_type": -1, "uid": "2480105654"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "海布里的机器猫", "province": 11, "statuses_count": 1153, "friends_count": 327, "city": 6, "text": "", "user_description": "", "mid": "3789429422006245", "verified_reason": "", "followers_count": 204, "parent": "3789423034434166", "t": 1418953841, "gender": "m", "verified": false, "verified_type": -1, "uid": "2536798750"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "太阳以西xi", "province": 100, "statuses_count": 676, "friends_count": 202, "city": 1000, "text": "", "user_description": "彦祖快到我碗里(⊙ω⊙)", "mid": "3789429628351486", "verified_reason": "", "followers_count": 217, "parent": "3789409982334652", "t": 1418953888, "gender": "f", "verified": false, "verified_type": -1, "uid": "1290757277"}, {"original_text": "//@星空下的巫师://@vinW: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "哀家很可爱1989", "province": 11, "statuses_count": 422, "friends_count": 71, "city": 5, "text": "", "user_description": "兴趣是最好的老师，努力是最靠谱的道路。", "mid": "3789429972097914", "verified_reason": "", "followers_count": 20, "parent": "3789422107939465", "t": 1418953972, "gender": "f", "verified": false, "verified_type": -1, "uid": "2055876571"}, {"original_text": "[赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "BigData大数据", "province": 100, "statuses_count": 6939, "friends_count": 506, "city": 1000, "text": "[赞]", "user_description": "博士，曾任职网易高级研究员和惠普云计算专家，大数据和深度学习研究者，CCF高级会员，CCF大数据专委会委员， 本微博所发表言论与任何单位无关！", "mid": "3789430475210377", "verified_reason": "网易 在线游戏事业部高级研究员", "followers_count": 17257, "parent": "3789423034434166", "t": 1418954092, "gender": "m", "verified": true, "verified_type": 0, "uid": "2870219257"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "风雨同路RUC", "province": 11, "statuses_count": 3867, "friends_count": 326, "city": 1000, "text": "", "user_description": "", "mid": "3789430621521393", "verified_reason": "", "followers_count": 146, "parent": "3789423034434166", "t": 1418954127, "gender": "m", "verified": false, "verified_type": -1, "uid": "2149271124"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "MRyliu", "province": 11, "statuses_count": 1954, "friends_count": 669, "city": 8, "text": "", "user_description": "", "mid": "3789430818688065", "verified_reason": "", "followers_count": 260, "parent": "3789423856244577", "t": 1418954174, "gender": "m", "verified": false, "verified_type": 220, "uid": "1836617214"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "anch3or", "province": 37, "statuses_count": 7573, "friends_count": 196, "city": 1, "text": "", "user_description": "若批评不自由，则赞美无意义。", "mid": "3789431401798006", "verified_reason": "", "followers_count": 62, "parent": "3789409982334652", "t": 1418954313, "gender": "m", "verified": false, "verified_type": -1, "uid": "2262061943"}, {"original_text": "//@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "LR机器学习计算机视觉", "province": 11, "statuses_count": 20932, "friends_count": 971, "city": 5, "text": "", "user_description": "北京工业大学软件学院讲师李蓉，专注计算机视觉和机器学习", "mid": "3789431624325174", "verified_reason": "", "followers_count": 6878, "parent": "3789430475210377", "t": 1418954366, "gender": "f", "verified": false, "verified_type": -1, "uid": "3209669611"}, {"original_text": " //@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "MF_Y", "province": 11, "statuses_count": 554, "friends_count": 483, "city": 1, "text": " ", "user_description": "mathematical finance, stochastic analysis, optimization", "mid": "3789431649600759", "verified_reason": "", "followers_count": 399, "parent": "3789423034434166", "t": 1418954372, "gender": "m", "verified": false, "verified_type": 220, "uid": "1308011730"}, {"original_text": "//@LR机器学习计算机视觉: //@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "胡聪_户主", "province": 11, "statuses_count": 928, "friends_count": 714, "city": 5, "text": "", "user_description": "近期专注于推荐系统，机器学习，数据挖掘 推荐系统论坛 http://rec-sys.net/ github：https://github.com/hu17889", "mid": "3789434363167707", "verified_reason": "", "followers_count": 418, "parent": "3789431624325174", "t": 1418955019, "gender": "m", "verified": false, "verified_type": -1, "uid": "1180535942"}, {"original_text": "//@vinW: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "畅领快意人生", "province": 31, "statuses_count": 35479, "friends_count": 428, "city": 7, "text": "", "user_description": "挨踢民工", "mid": "3789434518793263", "verified_reason": "", "followers_count": 631, "parent": "3789416407912601", "t": 1418955055, "gender": "f", "verified": false, "verified_type": -1, "uid": "1699100744"}, {"original_text": "@不匀称的猫 看人家都能搞十万小时了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "坚强爱吃米", "province": 31, "statuses_count": 35, "friends_count": 60, "city": 12, "text": "@不匀称的猫 看人家都能搞十万小时了", "user_description": "", "mid": "3789436036524874", "verified_reason": "", "followers_count": 55, "parent": "3789409982334652", "t": 1418955418, "gender": "m", "verified": false, "verified_type": -1, "uid": "1940896353"}, {"original_text": "//@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "海萌PI", "province": 11, "statuses_count": 658, "friends_count": 152, "city": 5, "text": "", "user_description": "Big Data 数据挖掘", "mid": "3789436267462087", "verified_reason": "", "followers_count": 64, "parent": "3789430475210377", "t": 1418955473, "gender": "m", "verified": false, "verified_type": -1, "uid": "3517246742"}, {"original_text": "//@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "yucheng_tianxia", "province": 61, "statuses_count": 1541, "friends_count": 304, "city": 1, "text": "", "user_description": "", "mid": "3789439858126393", "verified_reason": "", "followers_count": 136, "parent": "3789431624325174", "t": 1418956329, "gender": "m", "verified": false, "verified_type": -1, "uid": "1732883412"}, {"original_text": " //@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "lz周亮", "province": 11, "statuses_count": 1136, "friends_count": 280, "city": 8, "text": " ", "user_description": "爱出者爱返，福往者福来", "mid": "3789445821981867", "verified_reason": "", "followers_count": 539, "parent": "3789431624325174", "t": 1418957751, "gender": "m", "verified": false, "verified_type": -1, "uid": "1725145435"}, {"original_text": "//@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Tulip12358", "province": 51, "statuses_count": 5921, "friends_count": 107, "city": 11, "text": "", "user_description": "一场游戏一场梦，一个世界一个自己。", "mid": "3789446023710674", "verified_reason": "", "followers_count": 39, "parent": "3789431624325174", "t": 1418957799, "gender": "m", "verified": false, "verified_type": -1, "uid": "2649012443"}, {"original_text": "input 加noise和dropout的问题，一直没有太理解。//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "Stwrdy", "province": 400, "statuses_count": 2548, "friends_count": 74, "city": 15, "text": "input 加noise和dropout的问题，一直没有太理解。", "user_description": "I am a Keeper", "mid": "3789449835889616", "verified_reason": "", "followers_count": 68, "parent": "3789423856244577", "t": 1418958708, "gender": "m", "verified": false, "verified_type": -1, "uid": "2014541273"}, {"original_text": " @我的印象笔记  //@Yeah_Beautiful://@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "陈胖子_hh", "province": 44, "statuses_count": 531, "friends_count": 294, "city": 1000, "text": " @我的印象笔记  ", "user_description": "IDEA", "mid": "3789451237019534", "verified_reason": "", "followers_count": 97, "parent": "3789414515498493", "t": 1418959041, "gender": "m", "verified": false, "verified_type": -1, "uid": "2332653691"}, {"original_text": "//@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "机器学习与自然语言处理", "province": 11, "statuses_count": 658, "friends_count": 1593, "city": 8, "text": "", "user_description": "海量数据处理，云计算，机器学习，Linux,python..培训 http://www.peileyuan.com/", "mid": "3789453556298065", "verified_reason": "", "followers_count": 2898, "parent": "3789419750233644", "t": 1418959595, "gender": "f", "verified": false, "verified_type": -1, "uid": "2814648147"}, {"original_text": "//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "一个新教师", "province": 37, "statuses_count": 6504, "friends_count": 370, "city": 1, "text": "", "user_description": "", "mid": "3789455170733431", "verified_reason": "", "followers_count": 84, "parent": "3789453556298065", "t": 1418959979, "gender": "m", "verified": false, "verified_type": -1, "uid": "3957248256"}, {"original_text": "//@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Ronald_Xie", "province": 81, "statuses_count": 1563, "friends_count": 1093, "city": 1, "text": "", "user_description": "Robotics Scientist", "mid": "3789456408186560", "verified_reason": "", "followers_count": 182, "parent": "3789431624325174", "t": 1418960275, "gender": "m", "verified": false, "verified_type": -1, "uid": "2353979947"}, {"original_text": "//@winsty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "Ronald_Xie", "province": 81, "statuses_count": 1563, "friends_count": 1093, "city": 1, "text": "", "user_description": "Robotics Scientist", "mid": "3789462464866988", "verified_reason": "", "followers_count": 182, "parent": "3789422795710715", "t": 1418961718, "gender": "m", "verified": false, "verified_type": -1, "uid": "2353979947"}, {"original_text": "//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "fish2cat66", "province": 37, "statuses_count": 2309, "friends_count": 278, "city": 1, "text": "", "user_description": "", "mid": "3789462833742408", "verified_reason": "", "followers_count": 47, "parent": "3789453556298065", "t": 1418961807, "gender": "f", "verified": false, "verified_type": -1, "uid": "2918486431"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Super_Relcon", "province": 32, "statuses_count": 450, "friends_count": 277, "city": 2, "text": "", "user_description": "国家尚未统一，我等还有心刷微博、QQ", "mid": "3789467074381393", "verified_reason": "", "followers_count": 79, "parent": "3789409982334652", "t": 1418962818, "gender": "m", "verified": false, "verified_type": -1, "uid": "3023762595"}, {"original_text": "//@Ronald_Xie: //@winsty:大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "LR机器学习计算机视觉", "province": 11, "statuses_count": 20932, "friends_count": 971, "city": 5, "text": "", "user_description": "北京工业大学软件学院讲师李蓉，专注计算机视觉和机器学习", "mid": "3789470207366456", "verified_reason": "", "followers_count": 6878, "parent": "3789462464866988", "t": 1418963565, "gender": "f", "verified": false, "verified_type": -1, "uid": "3209669611"}, {"original_text": "//@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "张俊林say", "province": 11, "statuses_count": 1802, "friends_count": 1757, "city": 1000, "text": "", "user_description": "自然语言处理，搜索引擎，推荐系统，深度学习。", "mid": "3789471834951440", "verified_reason": "《这就是搜索引擎：核心技术详解》作者", "followers_count": 10322, "parent": "3789453556298065", "t": 1418963953, "gender": "m", "verified": true, "verified_type": 0, "uid": "1064649941"}, {"original_text": "//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "whsasuke", "province": 34, "statuses_count": 1822, "friends_count": 973, "city": 2, "text": "", "user_description": "", "mid": "3789472229343418", "verified_reason": "", "followers_count": 146, "parent": "3789471834951440", "t": 1418964047, "gender": "m", "verified": false, "verified_type": -1, "uid": "1820225141"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "wondertime", "province": 21, "statuses_count": 1672, "friends_count": 295, "city": 1000, "text": "", "user_description": "", "mid": "3789473999296020", "verified_reason": "", "followers_count": 203, "parent": "3789423856244577", "t": 1418964469, "gender": "m", "verified": false, "verified_type": -1, "uid": "1489956265"}, {"original_text": "//@张俊林say://@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regulariz", "username": "京酱宙斯W导导", "province": 11, "statuses_count": 666, "friends_count": 550, "city": 1, "text": "", "user_description": "偭蟂獭以隐处兮，夫岂从虾与蛭螾？", "mid": "3789475249563427", "verified_reason": "", "followers_count": 486, "parent": "3789471834951440", "t": 1418964766, "gender": "m", "verified": false, "verified_type": 220, "uid": "1972888773"}, {"original_text": "不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "回归自由2010", "province": 37, "statuses_count": 433, "friends_count": 318, "city": 2, "text": "不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了", "user_description": "", "mid": "3789476277152943", "verified_reason": "", "followers_count": 68, "parent": "3789471834951440", "t": 1418965012, "gender": "m", "verified": false, "verified_type": -1, "uid": "1898058401"}, {"original_text": "//@张俊林say: //@52nlp: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了", "username": "可爱的数据分析", "province": 11, "statuses_count": 1580, "friends_count": 1070, "city": 8, "text": "", "user_description": "负责优酷土豆个性化推荐系统架构和算法", "mid": "3789477627199392", "verified_reason": "", "followers_count": 2466, "parent": "3789471834951440", "t": 1418965334, "gender": "m", "verified": false, "verified_type": 220, "uid": "2363284975"}, {"original_text": "正在研究加噪声的机制。[挖鼻屎]//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "amay003", "province": 32, "statuses_count": 213, "friends_count": 250, "city": 1, "text": "正在研究加噪声的机制。[挖鼻屎]", "user_description": "", "mid": "3789480631101029", "verified_reason": "", "followers_count": 40, "parent": "3789409982334652", "t": 1418966050, "gender": "f", "verified": false, "verified_type": -1, "uid": "2674967851"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "砰砰的小屋", "province": 32, "statuses_count": 22409, "friends_count": 1790, "city": 2, "text": "", "user_description": "", "mid": "3789480651704199", "verified_reason": "", "followers_count": 511, "parent": "3789423034434166", "t": 1418966054, "gender": "f", "verified": false, "verified_type": -1, "uid": "2036923815"}, {"original_text": "//@刘知远THU://@鲁东东胖:不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//", "username": "砰砰的小屋", "province": 32, "statuses_count": 22409, "friends_count": 1790, "city": 2, "text": "", "user_description": "", "mid": "3789481167731758", "verified_reason": "", "followers_count": 511, "parent": "3789419209687105", "t": 1418966178, "gender": "f", "verified": false, "verified_type": -1, "uid": "2036923815"}, {"original_text": "//@张俊林say: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "数盟社区", "province": 11, "statuses_count": 7550, "friends_count": 970, "city": 8, "text": "", "user_description": "数据创造价值，数盟Q群：大数据 532494596，机器学习 463848712，数据分析 174306879，数据可视化 179287077，微信：DataScientistUnion", "mid": "3789483432744748", "verified_reason": "数盟社区 自媒体微博", "followers_count": 10308, "parent": "3789471834951440", "t": 1418966718, "gender": "m", "verified": true, "verified_type": 3, "uid": "3847741679"}, {"original_text": "@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "Hierarch1314", "province": 11, "statuses_count": 2103, "friends_count": 878, "city": 1000, "text": "@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了", "user_description": "", "mid": "3789484607117384", "verified_reason": "", "followers_count": 170, "parent": "3789483432744748", "t": 1418966998, "gender": "m", "verified": false, "verified_type": -1, "uid": "2145932032"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "泡泡茶bubbletea", "province": 33, "statuses_count": 1121, "friends_count": 280, "city": 1, "text": " ", "user_description": "", "mid": "3789488109205069", "verified_reason": "", "followers_count": 175, "parent": "3789409982334652", "t": 1418967833, "gender": "m", "verified": false, "verified_type": -1, "uid": "1472672341"}, {"original_text": "//@winsty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex", "username": "LR机器学习计算机视觉", "province": 11, "statuses_count": 20932, "friends_count": 971, "city": 5, "text": "", "user_description": "北京工业大学软件学院讲师李蓉，专注计算机视觉和机器学习", "mid": "3789488431999069", "verified_reason": "", "followers_count": 6878, "parent": "3789426608222700", "t": 1418967910, "gender": "f", "verified": false, "verified_type": -1, "uid": "3209669611"}, {"original_text": " //@张俊林say://@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "msnqqer", "province": 44, "statuses_count": 3127, "friends_count": 187, "city": 19, "text": " ", "user_description": "模式识别和机器学习爱好者，希望能够做点自己喜欢的事情", "mid": "3789489787193832", "verified_reason": "", "followers_count": 97, "parent": "3789471834951440", "t": 1418968232, "gender": "m", "verified": false, "verified_type": -1, "uid": "1654464704"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和DaMcAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "tvsofa2008", "province": 12, "statuses_count": 1635, "friends_count": 53, "city": 1, "text": "", "user_description": "", "mid": "3789490261190780", "verified_reason": "", "followers_count": 27, "parent": "3789423856244577", "t": 1418968345, "gender": "m", "verified": false, "verified_type": -1, "uid": "2344823017"}, {"original_text": "//@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "吴双SF", "province": 400, "statuses_count": 1391, "friends_count": 257, "city": 1, "text": "", "user_description": "一天到晚爬行的鱼", "mid": "3789491279942162", "verified_reason": "", "followers_count": 131, "parent": "3789430475210377", "t": 1418968589, "gender": "m", "verified": false, "verified_type": -1, "uid": "2445045141"}, {"original_text": "//@张俊林say: //@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的， 如果和L2等价，就变成一个convex regularization了//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization", "username": "行者行殇", "province": 43, "statuses_count": 11271, "friends_count": 177, "city": 1, "text": "", "user_description": "科研狗", "mid": "3789498493998711", "verified_reason": "", "followers_count": 51, "parent": "3789483432744748", "t": 1418970309, "gender": "m", "verified": false, "verified_type": -1, "uid": "3735078562"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "darkknightzh", "province": 42, "statuses_count": 1425, "friends_count": 24, "city": 1, "text": "", "user_description": "忘记一个人，从忘记那个声音开始", "mid": "3789502889572874", "verified_reason": "", "followers_count": 56, "parent": "3789423034434166", "t": 1418971357, "gender": "m", "verified": false, "verified_type": -1, "uid": "1212468547"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "加菲磕螺丝", "province": 31, "statuses_count": 7652, "friends_count": 626, "city": 1, "text": "", "user_description": "红专并进，理实交融", "mid": "3789538763807317", "verified_reason": "", "followers_count": 447, "parent": "3789423034434166", "t": 1418979910, "gender": "m", "verified": false, "verified_type": 220, "uid": "1659965751"}, {"original_text": "//@LR机器视觉: //@winty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex", "username": "东0610", "province": 32, "statuses_count": 3444, "friends_count": 28, "city": 1000, "text": "", "user_description": "没人知道我这个号，我可以毫无顾忌写自己心中所想！", "mid": "3789582744769192", "verified_reason": "", "followers_count": 73, "parent": "3789488431999069", "t": 1418990396, "gender": "m", "verified": false, "verified_type": -1, "uid": "1899089741"}, {"original_text": "//@LR机器学习计算机视觉://@Ronald_Xie: //@winsty:大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个co", "username": "Tulip12358", "province": 51, "statuses_count": 5921, "friends_count": 107, "city": 11, "text": "", "user_description": "一场游戏一场梦，一个世界一个自己。", "mid": "3789582799850913", "verified_reason": "", "followers_count": 39, "parent": "3789470207366456", "t": 1418990409, "gender": "m", "verified": false, "verified_type": -1, "uid": "2649012443"}, {"original_text": "//@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "说句VS心里话", "province": 22, "statuses_count": 6333, "friends_count": 668, "city": 1, "text": "", "user_description": "", "mid": "3789600059053890", "verified_reason": "", "followers_count": 135, "parent": "3789431624325174", "t": 1418994523, "gender": "m", "verified": false, "verified_type": -1, "uid": "1710412160"}, {"original_text": "转发微博。", "username": "岚京灵", "province": 11, "statuses_count": 981, "friends_count": 126, "city": 1, "text": "转发微博。", "user_description": "", "mid": "3789611119688725", "verified_reason": "", "followers_count": 21, "parent": "3789414675020515", "t": 1418997161, "gender": "f", "verified": false, "verified_type": -1, "uid": "1418771315"}, {"original_text": "//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "luweiming", "province": 33, "statuses_count": 5270, "friends_count": 136, "city": 1, "text": "", "user_description": "", "mid": "3789622037449517", "verified_reason": "", "followers_count": 64, "parent": "3789409982334652", "t": 1418999764, "gender": "m", "verified": false, "verified_type": -1, "uid": "1938575460"}, {"original_text": "//@winsty:大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "UCAS-王剑", "province": 42, "statuses_count": 1140, "friends_count": 566, "city": 1, "text": "", "user_description": "", "mid": "3789631587725558", "verified_reason": "", "followers_count": 106, "parent": "3789470207366456", "t": 1419002041, "gender": "m", "verified": false, "verified_type": -1, "uid": "2946886455"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "itolssy", "province": 12, "statuses_count": 1827, "friends_count": 125, "city": 4, "text": "", "user_description": "", "mid": "3789638655498172", "verified_reason": "", "followers_count": 595, "parent": "3789423407374682", "t": 1419003726, "gender": "m", "verified": false, "verified_type": 220, "uid": "1164715021"}, {"original_text": "规律可循......//@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "wishchin", "province": 100, "statuses_count": 574, "friends_count": 72, "city": 1000, "text": "规律可循......", "user_description": "", "mid": "3789646511079862", "verified_reason": "", "followers_count": 28, "parent": "3789409982334652", "t": 1419005599, "gender": "m", "verified": false, "verified_type": -1, "uid": "3686670990"}, {"original_text": "//@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "爱生活爱阿慌", "province": 46, "statuses_count": 2024, "friends_count": 335, "city": 1, "text": "", "user_description": "爱生活爱阿慌，科科科", "mid": "3789651976118093", "verified_reason": "", "followers_count": 235, "parent": "3789431624325174", "t": 1419006902, "gender": "m", "verified": false, "verified_type": 220, "uid": "1616292410"}, {"original_text": "//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "贝壳ustb", "province": 11, "statuses_count": 282, "friends_count": 70, "city": 8, "text": "", "user_description": "好人。", "mid": "3789655968847160", "verified_reason": "", "followers_count": 52, "parent": "3789423034434166", "t": 1419007854, "gender": "m", "verified": false, "verified_type": -1, "uid": "2310375375"}, {"original_text": "//@LR机器学习计算机视觉: //@winsty:我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex", "username": "Yihsiung_CSU", "province": 43, "statuses_count": 1891, "friends_count": 699, "city": 1, "text": "", "user_description": "", "mid": "3789817022733689", "verified_reason": "", "followers_count": 160, "parent": "3789488431999069", "t": 1419046251, "gender": "m", "verified": false, "verified_type": -1, "uid": "2607492470"}, {"original_text": "//@winsty: 我接着来补充，大家可以去看看chen minmin今年的icml marginalized denoising autoencoder相关。一大缺憾就是现在没人能给出relu的分析//@鲁东东胖: 不太准确，Percy Liang和David McAllester等都有文章/talk, 还是比L2要更adaptive的，如果和L2等价，就变成一个convex regularization了", "username": "_ml_fans", "province": 51, "statuses_count": 3162, "friends_count": 540, "city": 6, "text": "", "user_description": "读paper而不coding则罔，只coding而不啃paper则殆", "mid": "3789836848511006", "verified_reason": "", "followers_count": 842, "parent": "3789422795710715", "t": 1419050979, "gender": "m", "verified": false, "verified_type": -1, "uid": "1717217073"}, {"original_text": "//@老师木: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "乔鹏飞BIT", "province": 11, "statuses_count": 1234, "friends_count": 88, "city": 8, "text": "", "user_description": "", "mid": "3789911104367430", "verified_reason": "", "followers_count": 33, "parent": "3789423034434166", "t": 1419068683, "gender": "m", "verified": false, "verified_type": -1, "uid": "2106427232"}, {"original_text": "//@王威廉: Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "ryanzpn", "province": 31, "statuses_count": 2409, "friends_count": 290, "city": 10, "text": "", "user_description": "make it possible", "mid": "3790155166548579", "verified_reason": "", "followers_count": 169, "parent": "3789409982334652", "t": 1419126872, "gender": "m", "verified": false, "verified_type": -1, "uid": "1188013941"}, {"original_text": " //@强闰伟_LocalsNake://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "咩咩羊多动症患者", "province": 11, "statuses_count": 1463, "friends_count": 764, "city": 8, "text": " ", "user_description": "", "mid": "3791360467327902", "verified_reason": "", "followers_count": 610, "parent": "3789423613358020", "t": 1419414238, "gender": "f", "verified": false, "verified_type": 220, "uid": "1842121641"}, {"original_text": "//@对多动症儿童: //@强闰伟_LocalsNake://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "___Oscar", "province": 42, "statuses_count": 1120, "friends_count": 315, "city": 1, "text": "", "user_description": "早起的虫儿有鸟吃", "mid": "3791422539385405", "verified_reason": "", "followers_count": 205, "parent": "3791360467327902", "t": 1419429037, "gender": "m", "verified": false, "verified_type": -1, "uid": "1858240077"}, {"original_text": "//@说句VS心里话://@LR机器学习计算机视觉://@BigData大数据: [赞]//@老师木://@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是", "username": "说句VS心里话", "province": 22, "statuses_count": 6333, "friends_count": 668, "city": 1, "text": "", "user_description": "", "mid": "3796629217407004", "verified_reason": "", "followers_count": 135, "parent": "3789600059053890", "t": 1420670406, "gender": "m", "verified": false, "verified_type": -1, "uid": "1710412160"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "mao_thu", "province": 32, "statuses_count": 396, "friends_count": 292, "city": 1, "text": " ", "user_description": "", "mid": "3797068956611212", "verified_reason": "", "followers_count": 72, "parent": "3789409982334652", "t": 1420775248, "gender": "f", "verified": false, "verified_type": -1, "uid": "1925618333"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "JoinWei_", "province": 11, "statuses_count": 339, "friends_count": 196, "city": 8, "text": " ", "user_description": "AI观察员 北大渣硕", "mid": "3797628980987789", "verified_reason": "", "followers_count": 48, "parent": "3789409982334652", "t": 1420908768, "gender": "m", "verified": false, "verified_type": -1, "uid": "3223881890"}, {"original_text": " //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "Claire谷", "province": 400, "statuses_count": 866, "friends_count": 157, "city": 1, "text": " ", "user_description": "", "mid": "3807987255041069", "verified_reason": "", "followers_count": 75, "parent": "3789409982334652", "t": 1423378373, "gender": "f", "verified": false, "verified_type": -1, "uid": "2715821621"}, {"original_text": "//@Claire谷: //@王威廉:Hinton自己说是feature co-adaptation，不过2013年的时候，已有NIPS文章指出dropout在GLM上是等价于L2 regularization//@梁斌penny: 之前和一谷歌美军交流，input加噪音相当于regularzation，dropout 解决的是co-adaption的问题", "username": "曹彦君", "province": 400, "statuses_count": 930, "friends_count": 365, "city": 1, "text": "", "user_description": "不疯魔，不成活", "mid": "3808235234896920", "verified_reason": "", "followers_count": 348, "parent": "3807987255041069", "t": 1423437496, "gender": "f", "verified": false, "verified_type": 220, "uid": "1522464137"}]